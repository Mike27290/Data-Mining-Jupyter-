{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-IE-pEY-0Og"
   },
   "source": [
    "## Using Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fmarciaturner%2Ffiles%2F2018%2F01%2FWegmans-Produce-1.jpg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent Itemsets via Apriori Algorithm\n",
    "Apriori function to extract frequent itemsets for association rule mining\n",
    "We have a dataset of a mall with 7500 transactions of different customers buying different items from the store.\n",
    "We have to find correlations between the different items in the store. so that we can know if a customer is buying apple, banana and mango. what is the next item, The customer would be interested in buying from the store. \n",
    "\n",
    "## Overview\n",
    "Apriori is a popular algorithm for extracting frequent itemsets with applications in association rule learning. The apriori algorithm has been designed to operate on databases containing transactions, such as purchases by customers of a store. An itemset is considered as \"frequent\" if it meets a user-specified support threshold. For instance, if the support threshold is set to 0.5 (50%), a frequent itemset is defined as a set of items that occur together in at least 50% of all transactions in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RODwj2Ug-9lH"
   },
   "source": [
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# !pip install squarify\n",
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "colab": {},
    "colab_type": "code",
    "id": "72zscNfA-sYt"
   },
   "outputs": [],
   "source": [
    "# for basic operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# for defining path\n",
    "import os\n",
    "# print(os.listdir('../input/'))\n",
    "\n",
    "# for market basket analysis\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP0H9a4pAg-p"
   },
   "source": [
    "**Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "OTEXbC-TAfVO",
    "outputId": "a55b4f7a-9995-46d0-b431-1978e4170f49"
   },
   "outputs": [],
   "source": [
    "# reading the dataset\n",
    "\n",
    "data = pd.read_csv('store_transaction.csv', header = None)\n",
    "\n",
    "# let's check the shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# checking the head of the data\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkng the tail of the data\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the random entries in the data\n",
    "\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "colab_type": "code",
    "id": "g3qz9RBeBfSY",
    "outputId": "56093766-a673-460d-ec1b-282c3e0b2547"
   },
   "outputs": [],
   "source": [
    "# let's describe the dataset\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elements = data.values.tolist()\n",
    "flat_list = [item for sublist in all_elements for item in sublist]\n",
    "cleanedList = [x for x in flat_list if str(x) != 'nan']\n",
    "temp_df = pd.DataFrame({'col':cleanedList})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "colab_type": "code",
    "id": "5WEHLSXdcrvo",
    "outputId": "0c261675-8469-4367-f9ab-fdcb489fdefb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 15)\n",
    "wordcloud = WordCloud(background_color = 'white', width = 1200,  height = 1200, max_words = 25).generate(str(cleanedList))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Most Popular Items',fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "id": "gM0kNchXdlZN",
    "outputId": "ccb83a24-9c67-41ea-a452-050650f328b9"
   },
   "outputs": [],
   "source": [
    "# looking at the frequency of most popular items \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 7)\n",
    "color = plt.cm.copper(np.linspace(0, 1, 40))\n",
    "temp_df['col'].value_counts().head(40).plot.bar(color = color)\n",
    "plt.title('frequency of most popular items', fontsize = 20)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = temp_df['col'].value_counts().head(50).to_frame()\n",
    "\n",
    "# plotting a tree map\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "color = plt.cm.cool(np.linspace(0, 1, 50))\n",
    "squarify.plot(sizes = y.values, label = y.index, alpha=.8, color = color)\n",
    "plt.title('Tree Map for Popular Items')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CbarYbRvBzdw"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making each customers shopping items an identical list\n",
    "trans = []\n",
    "for i in range(0, 7501):\n",
    "    trans.append([str(data.values[i,j]) for j in range(0, 20)])\n",
    "\n",
    "# conveting it into an numpy array\n",
    "trans = np.array(trans)\n",
    "\n",
    "# checking the shape of the array\n",
    "print(trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transaction encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# To know more about TransactionEncoder go to  http://rasbt.github.io/mlxtend/user_guide/preprocessing/TransactionEncoder/\n",
    "\n",
    "te = TransactionEncoder()\n",
    "data = te.fit_transform(trans)\n",
    "data = pd.DataFrame(data, columns = te.columns_)\n",
    "\n",
    "# getting the shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the head of the data\n",
    "\n",
    "del data['nan']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2017/03/Apriori-Algorithm.jpg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Apriori Algorithm Work ?\n",
    "\n",
    "A key concept in Apriori algorithm is the anti-monotonicity of the support measure. It assumes that\n",
    "\n",
    "* All subsets of a frequent itemset must be frequent\n",
    "* Similarly, for any infrequent itemset, all its supersets must be infrequent too\n",
    "\n",
    "**Step 1**: Create a frequency table of all the items that occur in all the transactions.\n",
    "\n",
    "**Step 2**: We know that only those elements are significant for which the support is greater than or equal to the threshold support.\n",
    "\n",
    "**Step 3**: The next step is to make all the possible pairs of the significant items keeping in mind that the order doesn’t matter, i.e., AB is same as BA.\n",
    "\n",
    "**Step 4**: We will now count the occurrences of each pair in all the transactions.\n",
    "\n",
    "**Step 5**: Again only those itemsets are significant which cross the support threshold\n",
    "\n",
    "**Step 6**: Now let’s say we would like to look for a set of three items that are purchased together. We will use the itemsets found in step 5 and create a set of 3 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "#Now, let us return the items and itemsets with at least 1% support:\n",
    "apriori(data, min_support = 0.01, use_colnames = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of working with pandas DataFrames is that we can use its convenient features to filter the results. For instance, let's assume we are only interested in itemsets of length 2 that have a support of at least 80 percent. First, we create the frequent itemsets via apriori and add a new column that stores the length of each itemset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and Filtering the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(data, min_support = 0.01, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting th item sets with length = 2 and support more han 1%\n",
    "support_threhold = 0.01\n",
    "length = 2\n",
    "\n",
    "frequent_itemsets = apriori(data, min_support = support_threhold, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "print (frequent_itemsets[(frequent_itemsets['length'] == length) &\n",
    "                   (frequent_itemsets['support'] >= support_threhold) ])\n",
    "print (frequent_itemsets[(frequent_itemsets['length'] == length) &\n",
    "                   (frequent_itemsets['support'] >= support_threhold) ].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[frequent_itemsets['itemsets'] == {'mineral water'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[frequent_itemsets['itemsets'] == {'eggs'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[frequent_itemsets['itemsets'] == {'Spaghetti'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[frequent_itemsets['itemsets'] == {'chocolate'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[frequent_itemsets['itemsets'] == {'chocolate', 'mineral water'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is adaption from the following kaggle notebook https://www.kaggle.com/roshansharma/market-basket-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Market_Basket.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
